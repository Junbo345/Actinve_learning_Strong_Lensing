{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RND0zrMvgmpi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "Paar-riPhUYI",
        "outputId": "1bc312cf-35ea-4732-fe48-5722dd247e87"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6baf7641-e791-431f-9dd6-4489d2555cfa\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6baf7641-e791-431f-9dd6-4489d2555cfa\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving strong_lense_train.csv to strong_lense_train.csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlgM476CO8vy",
        "outputId": "3ebbee7f-a4ca-4f42-fe67-34bb2b31ffcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3329\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "df = pd.read_csv('strong_lense_train.csv')\n",
        "print(len(df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SgEtSDTqILy"
      },
      "outputs": [],
      "source": [
        "# catalog = pd.read_csv('q1_discovery_engine_lens_catalog(1).csv')\n",
        "catalog = df\n",
        "catalog['expert_score'] = catalog['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "-CHJ77JrqygE",
        "outputId": "5816ab5a-b7ed-4c47-d130-f32a21cddd27"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       102018212_-588199575513249872\n",
              "1       102018213_-594971757513738267\n",
              "2       102018664_-563268036507955941\n",
              "3       102018664_-566148966507539414\n",
              "4       102018664_-569003084507675615\n",
              "                    ...              \n",
              "3324    102160612_2745259655683405723\n",
              "3325    102160612_2745634053683302741\n",
              "3326    102160612_2745727028683190439\n",
              "3327    102160612_2746348900682544986\n",
              "3328    102160876_2732221892687914375\n",
              "Name: id_str, Length: 3329, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id_str</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>102018212_-588199575513249872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>102018213_-594971757513738267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>102018664_-563268036507955941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>102018664_-566148966507539414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>102018664_-569003084507675615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3324</th>\n",
              "      <td>102160612_2745259655683405723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3325</th>\n",
              "      <td>102160612_2745634053683302741</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3326</th>\n",
              "      <td>102160612_2745727028683190439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3327</th>\n",
              "      <td>102160612_2746348900682544986</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3328</th>\n",
              "      <td>102160876_2732221892687914375</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3329 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "catalog['id_str']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2VKtYFsqD0F",
        "outputId": "296b815c-6826-4136-a29f-9869db5e16db"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3329"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "\n",
        "catalog['id_str'] = catalog['id_str'].str.replace('NEG', '-')\n",
        "df = pd.merge(df, catalog[['id_str', 'expert_score']], how='inner')\n",
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "V8Op3eohqIpt",
        "outputId": "53cad5d5-a172-49c3-9599-b687d54581db"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       102018212_-588199575513249872\n",
              "1       102018213_-594971757513738267\n",
              "2       102018664_-563268036507955941\n",
              "3       102018664_-566148966507539414\n",
              "4       102018664_-569003084507675615\n",
              "                    ...              \n",
              "3324    102160612_2745259655683405723\n",
              "3325    102160612_2745634053683302741\n",
              "3326    102160612_2745727028683190439\n",
              "3327    102160612_2746348900682544986\n",
              "3328    102160876_2732221892687914375\n",
              "Name: id_str, Length: 3329, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id_str</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>102018212_-588199575513249872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>102018213_-594971757513738267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>102018664_-563268036507955941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>102018664_-566148966507539414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>102018664_-569003084507675615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3324</th>\n",
              "      <td>102160612_2745259655683405723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3325</th>\n",
              "      <td>102160612_2745634053683302741</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3326</th>\n",
              "      <td>102160612_2745727028683190439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3327</th>\n",
              "      <td>102160612_2746348900682544986</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3328</th>\n",
              "      <td>102160876_2732221892687914375</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3329 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "df['id_str']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcmQbW_IQiY3"
      },
      "outputs": [],
      "source": [
        "# (df['expert_score'] > 1.5).sum()\n",
        "# df['label'].value_counts()\n",
        "\n",
        "# I think you used > 1.5 as a lens? numbers match (almost)\n",
        "\n",
        "label_bin_cutoff = 0.5\n",
        "df['label'] = df['expert_score'] > label_bin_cutoff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_WaP701PwwO"
      },
      "outputs": [],
      "source": [
        "\n",
        "# just to help with class imbalance, drop the bottom 50%?\n",
        "\n",
        "# import numpy as np\n",
        "# expert_score_cut = np.percentile(df['expert_score'], 50)\n",
        "\n",
        "# df = df[df['expert_score'] > expert_score_cut].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWBFZZ5OheT0"
      },
      "outputs": [],
      "source": [
        "\n",
        "X = df[[col for col in df.columns.values if col.startswith('feat')]].values\n",
        "# y = df['expert_score'] / df['expert_score'].max()\n",
        "y = df['expert_score'].values\n",
        "\n",
        "\n",
        "# Separate features X (first 40 columns) and target y (the 41st column)\n",
        "# X = df.iloc[:, 1:41].values   # shape: (N, 40)\n",
        "# y = df.iloc[:, 41].values    # shape: (N,)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEay8G3zPfkN"
      },
      "outputs": [],
      "source": [
        "# y.min(), y.max()\n",
        "# import matplotlib.pyplot as plt\n",
        "# _ = plt.hist(y)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vxEl5ZrOrZW"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Train-test split (no stratify to avoid errors with extremely small classes)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.3,\n",
        "    random_state=38\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "5Bo6w_bfPjMg",
        "outputId": "691f19be-75f6-4b07-a676-bde8637449f4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-42-fea0e6d8b5a8>:5: FutureWarning: pandas.value_counts is deprecated and will be removed in a future version. Use pd.Series(obj).value_counts() instead.\n",
            "  pd.value_counts(y_train_class)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>False</th>\n",
              "      <td>2143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>True</th>\n",
              "      <td>187</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ],
            "text/plain": [
              "False    2143\n",
              "True      187\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# and the classification equivalent\n",
        "y_train_class = y_train > label_bin_cutoff\n",
        "y_test_class = y_test > label_bin_cutoff\n",
        "\n",
        "pd.value_counts(y_train_class)\n",
        "# pd.value_counts(y_test_class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPZ7rqECQTwX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUp6e7o5dTo4",
        "outputId": "b99e0365-4280-493d-b6df-67a04315d807"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Out of the top 100 highest probability samples, 38 are actually class 1.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define simple MLP model (1 hidden layer, 128 units)\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(128,), activation='relu', solver='adam', max_iter=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "mlp.fit(X_train, y_train_class)\n",
        "\n",
        "# Predict probabilities for the test set\n",
        "y_pred_probs = mlp.predict_proba(X_test)[:, 1]  # Probabilities for class 1\n",
        "\n",
        "# Get the top 100 samples with the highest probabilities\n",
        "top_100_indices = np.argsort(-y_pred_probs)[:100]  # Sort in descending order\n",
        "top_100_labels = y_test_class[top_100_indices]  # Get actual class labels of top 100\n",
        "\n",
        "# Count how many of the top 100 are actually class 1\n",
        "num_class_1 = np.sum(top_100_labels)\n",
        "\n",
        "# Print results\n",
        "print(f\"Out of the top 100 highest probability samples, {num_class_1} are actually class 1.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZAy1xUedUuj"
      },
      "source": [
        "#**Baseline model, cross entropy nn**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izoY3dKKRtPA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXZ6lOtVrBe8"
      },
      "source": [
        "#**Focal loss**\n",
        "\n",
        "\n",
        "$$\n",
        "FL(p_t) = -\\alpha_t (1-p_t)^\\gamma \\log(p_t)\n",
        "$$\n",
        "\n",
        "**here $-α$ is -0.75 when y = 1 and -0.25 when y = 0.\n",
        "$γ$ is 2.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5HlN9uFt9Er",
        "outputId": "dfb0bce5-358b-4d27-c58f-0f0b7c46e658"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50, Avg Train Loss: 0.0001\n",
            "Epoch 100, Avg Train Loss: 0.0000\n",
            "Epoch 150, Avg Train Loss: 0.0000\n",
            "Epoch 200, Avg Train Loss: 0.0000\n",
            "Epoch 250, Avg Train Loss: 0.0000\n",
            "Epoch 300, Avg Train Loss: 0.0000\n",
            "Epoch 350, Avg Train Loss: 0.0000\n",
            "Epoch 400, Avg Train Loss: 0.0000\n",
            "Epoch 450, Avg Train Loss: 0.0000\n",
            "Epoch 500, Avg Train Loss: 0.0000\n",
            "Test Loss: 0.0000\n",
            "Out of the top 100 highest probability samples, 42.0 are actually class 1.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train_class, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test_class, dtype=torch.float32)\n",
        "\n",
        "# Define a simple MLP\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=128):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, 1)  # Output a single score (logit)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x  # Raw logits (we'll apply sigmoid in loss function)\n",
        "\n",
        "# Implement Focal Loss\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.25, gamma=2.0):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.bce = nn.BCEWithLogitsLoss(reduction='none')  # We compute loss manually\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        \"\"\" Compute Focal Loss for binary classification \"\"\"\n",
        "        targets = targets.view(-1)  # 🔥 Ensure targets have shape (batch_size,)\n",
        "        probs = torch.sigmoid(logits)  # Convert logits to probabilities\n",
        "\n",
        "        # Compute standard BCE loss\n",
        "        bce_loss = self.bce(logits, targets)\n",
        "\n",
        "        # Compute focal loss term\n",
        "        pt = probs * targets + (1 - probs) * (1 - targets)  # P_t = P if y=1, 1-P if y=0\n",
        "        focal_weight = (1 - pt) ** self.gamma\n",
        "\n",
        "        # Apply alpha-balancing factor\n",
        "        alpha_factor = self.alpha * targets + (1 - self.alpha) * (1 - targets)\n",
        "\n",
        "        # Final loss\n",
        "        focal_loss = alpha_factor * focal_weight * bce_loss\n",
        "        return focal_loss.mean()\n",
        "\n",
        "# Initialize model, optimizer, and loss function\n",
        "model = MLP(input_dim=40, hidden_dim=64)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = FocalLoss(alpha=0.25, gamma=2.0)\n",
        "# Training loop\n",
        "epochs = 500\n",
        "batch_size = 128\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    # Mini-batch training\n",
        "    for i in range(0, len(X_train_tensor), batch_size):\n",
        "        batch_X = X_train_tensor[i:i+batch_size]\n",
        "        batch_y = y_train_tensor[i:i+batch_size]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(batch_X).squeeze()  # Get raw scores (logits)\n",
        "        loss = criterion(logits, batch_y.view(-1))  # 🔥 Fix shape issue\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    if (epoch + 1) % 50 == 0:\n",
        "        print(f\"Epoch {epoch+1}, Avg Train Loss: {total_loss / (len(X_train_tensor)):.4f}\")\n",
        "\n",
        "# Evaluation: Get probabilities and rank top 100\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    logits_test = model(X_test_tensor).squeeze()\n",
        "    probs_test = torch.sigmoid(logits_test).numpy()  # Convert logits to probabilities\n",
        "\n",
        "test_loss = criterion(logits_test, y_test_tensor)\n",
        "print(f'Test Loss: {total_loss / (len(logits_test) // batch_size):.4f}')\n",
        "\n",
        "# Get the top 100 samples with the highest probabilities\n",
        "top_100_indices = np.argsort(-probs_test)[:100]  # Sort in descending order\n",
        "top_100_labels = y_test_class[top_100_indices]  # Get actual class labels of top 100\n",
        "\n",
        "# Count how many of the top 100 are actually class 1\n",
        "num_class_1 = np.sum(top_100_labels)\n",
        "\n",
        "# Print results\n",
        "print(f\"Out of the top 100 highest probability samples, {num_class_1} are actually class 1.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRlP1gz2sDrz"
      },
      "source": [
        "# **Margin Ranking Loss**\n",
        "\n",
        "Mathematically, Margin Ranking Loss is defined on **pairs of examples** with a label indicating their desired order. For a single pair of items with model output scores $x_1$ and $x_2$, and a target label $y \\in \\{1, -1\\}$, the **per-pair** loss is given by:\n",
        "\n",
        "$$\n",
        "L(x_1, x_2, y) = \\max (0, -y \\cdot (x_1 - x_2) + m).\n",
        "$$\n",
        "\n",
        "### **Explanation of the Formula**\n",
        "- **$x_1, x_2$**: The scores predicted by the model for the two items being compared. A higher score means the item is ranked more highly (e.g., more relevant or more preferred).\n",
        "\n",
        "- **$y$**: The **label** indicating the correct ranking for the pair. Conventionally:\n",
        "  - $y = 1$ if **item 1 should be ranked higher** than item 2.\n",
        "  - $y = -1$ if **item 2 should be ranked higher** than item 1.\n",
        "\n",
        "  In other words, $y$ tells us which item is supposed to \"win\" in the comparison.\n",
        "\n",
        "- **$m$**: The **margin** (a non-negative hyperparameter) that defines the required difference between scores.\n",
        "  - If the higher-ranked item’s score is at least **$m$** greater than the lower-ranked item’s score, the loss for that pair will be **zero**.\n",
        "  - A margin of $m = 0$ means we only care about getting the order correct.\n",
        "  - A positive margin **enforces a gap** between the scores, ensuring the ranking is not only correct but also has sufficient separation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGk2jJTahrf9"
      },
      "outputs": [],
      "source": [
        "class PairwiseRankingDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        \"\"\"\n",
        "        X: NumPy array of shape (N, D)\n",
        "        y: NumPy array of shape (N,)\n",
        "        num_pairs: number of pairs to sample\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_samples = len(X)\n",
        "\n",
        "        self.X = torch.tensor(X, dtype=torch.float)\n",
        "        self.y = torch.tensor(y, dtype=torch.float)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Randomly get two samples\n",
        "        indices = np.random.choice(range(self.num_samples), size=2, replace=False)\n",
        "        i = indices[0]\n",
        "        j = indices[1]\n",
        "\n",
        "        x_i = self.X[i]  # shape: (40,)\n",
        "        x_j = self.X[j]  # shape: (40,)\n",
        "\n",
        "        y_i = self.y[i]\n",
        "        y_j = self.y[j]\n",
        "\n",
        "        # Our margin-ranking 'label' is always +1 => x_i should rank higher than x_j.\n",
        "        target = 1 if y_i > y_j else 0\n",
        "\n",
        "\n",
        "\n",
        "        return x_i, x_j, target\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3GLf7fbhs88"
      },
      "outputs": [],
      "source": [
        "train_dataset = PairwiseRankingDataset(X_train, y_train)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "test_dataset = PairwiseRankingDataset(X_test, y_test)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hD09fq54SGhy",
        "outputId": "1bb8d8b4-afee-48f5-ff3c-a22ac94496da"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[tensor([[-7.7379e+00, -7.1560e-01,  1.0268e+00,  ...,  6.4300e-01,\n",
              "          -4.8105e-01,  2.5116e-01],\n",
              "         [-2.5668e+00,  5.4763e+00,  3.9950e+00,  ..., -1.3153e+00,\n",
              "          -1.7775e+00,  8.5259e-01],\n",
              "         [-1.6074e+00,  4.8418e+00, -5.2429e+00,  ..., -6.0304e-01,\n",
              "          -1.3523e-03, -8.5678e-02],\n",
              "         ...,\n",
              "         [-1.3567e+00, -2.7922e+00,  4.3208e+00,  ...,  7.4573e-01,\n",
              "          -1.6440e-01, -6.0536e-01],\n",
              "         [-6.7275e+00,  4.5620e+00, -1.0955e+00,  ..., -9.0227e-02,\n",
              "           4.2628e-01,  2.0157e-01],\n",
              "         [-1.0888e+00, -1.8525e+00, -1.5739e-01,  ..., -1.8716e-01,\n",
              "           8.4380e-01,  5.0495e-01]]),\n",
              " tensor([[-8.3106,  0.5640,  0.2281,  ...,  0.8582, -0.1769, -0.1098],\n",
              "         [ 3.5834,  8.9819,  8.5525,  ..., -0.8339,  0.1853,  0.6552],\n",
              "         [-3.4402,  5.6220,  2.1022,  ...,  0.1987,  0.4750, -0.0384],\n",
              "         ...,\n",
              "         [ 2.7290, -6.0325,  4.2567,  ...,  0.3401,  1.0115,  0.0376],\n",
              "         [-5.1121,  2.4035, -4.2935,  ..., -0.3206, -0.1527,  0.2766],\n",
              "         [-0.0286,  1.7082,  5.4528,  ...,  2.1497, -1.6023, -2.1857]]),\n",
              " tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,\n",
              "         0, 1, 0, 0, 0, 1, 0, 1])]"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "next(iter(train_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqZE6_yXhxtW"
      },
      "outputs": [],
      "source": [
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self, input_dim=40, hidden_dim=128):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1)   # single output -> \"score\"\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ga2nc5fZh2xR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9ZJWNAQSAA4",
        "outputId": "e54c8127-94b6-45bf-c5c1-8cda92360b2a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(np.float64(0.0), np.float64(1.0))"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train.min(), y_train.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "biSraU0_h-v7",
        "outputId": "f016327c-eb3a-4665-8e4e-172304475aa3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 50, Loss: 0.1020\n",
            "Epoch 100, Loss: 0.0265\n",
            "Epoch 150, Loss: 0.0033\n",
            "Epoch 200, Loss: 0.0005\n",
            "Epoch 250, Loss: 0.0028\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Instantiate the model\n",
        "model = SimpleMLP(input_dim=40, hidden_dim=128)\n",
        "\n",
        "criterion = nn.MarginRankingLoss(margin=0.)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "num_epochs = 250\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0.0\n",
        "    for batch in train_loader:\n",
        "        x_i_batch, x_j_batch, target_batch = batch\n",
        "        # x_i_batch: (batch_size, 40)\n",
        "        # x_j_batch: (batch_size, 40)\n",
        "        # target_batch: should be (batch_size,) after we fix below.\n",
        "\n",
        "        # 1. Forward pass\n",
        "        score_i = model(x_i_batch).squeeze()  # shape: (batch_size,)\n",
        "        score_j = model(x_j_batch).squeeze()  # shape: (batch_size,)\n",
        "\n",
        "        # 2. Fix the shape of target_batch\n",
        "        # If target_batch is (batch_size, 1), we can squeeze it:\n",
        "        target_batch = target_batch.squeeze()  # shape: (batch_size,)\n",
        "\n",
        "        # 3. Compute loss\n",
        "        loss = criterion(score_i, score_j, target_batch)\n",
        "\n",
        "        # 4. Backprop\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    if (epoch + 1) % 50 == 0:\n",
        "        print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLRfdrRUjlqP",
        "outputId": "d6ac9445-889c-4f94-b655-30b87e0217a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of samples in top-100 that are actual class=1: 6\n",
            "Fraction correct among top-100: 0.06\n",
            "0.0037090007681399584\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "\n",
        "# Convert X_test to torch\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float)\n",
        "\n",
        "test_loss = 0.\n",
        "for batch in test_loader:\n",
        "    x_i_batch, x_j_batch, target_batch = batch\n",
        "    score_i = model(x_i_batch).squeeze()  # shape: (batch_size,)\n",
        "    score_j = model(x_j_batch).squeeze()  # shape: (batch_size,)\n",
        "    target_batch = target_batch.squeeze()  # shape: (batch_size,)\n",
        "    loss = criterion(score_i, score_j, target_batch)\n",
        "    test_loss += loss.item()\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    scores_test = model(X_test_tensor).squeeze().numpy()  # shape: (N_test,)\n",
        "\n",
        "# Sort test samples by descending score\n",
        "sorted_indices = np.argsort(scores_test)[::-1]\n",
        "\n",
        "top_100_indices = sorted_indices[:100]\n",
        "\n",
        "X_test_top_100 = X_test[top_100_indices]\n",
        "y_test_top_100 = y_test[top_100_indices]\n",
        "scores_top_100 = scores_test[top_100_indices]\n",
        "\n",
        "# Count how many of these top 100 are actually class 1\n",
        "num_correct_in_top_100 = np.sum(y_test_top_100 > label_bin_cutoff)\n",
        "\n",
        "print(\"Number of samples in top-100 that are actual class=1:\", num_correct_in_top_100)\n",
        "print(\"Fraction correct among top-100:\", num_correct_in_top_100 / 100)\n",
        "print(test_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4FA416zsW_h",
        "outputId": "8e8e1033-06f2-440d-8e4a-9acf78a663a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.03703704, 0.        , 0.        , 0.28057653, 0.03333334,\n",
              "       0.13333334, 0.06666667, 0.        , 0.        , 0.15815203,\n",
              "       0.38345939, 0.08333334, 0.04166667, 0.23378076, 0.21618381,\n",
              "       0.3168126 , 0.        , 0.24735722, 0.        , 0.38841432,\n",
              "       0.23521318, 0.03333334, 0.28436854, 0.06666667, 0.3091011 ,\n",
              "       0.        , 0.59802109, 0.17782824, 0.19005062, 0.25534552,\n",
              "       0.38067093, 0.13333334, 0.03333334, 0.        , 0.1235102 ,\n",
              "       0.        , 0.0952381 , 0.32254475, 0.32614326, 0.27308294,\n",
              "       0.1       , 0.23671941, 0.03333334, 0.        , 0.45220011,\n",
              "       0.13333334, 0.1968489 , 0.24341507, 0.15600733, 0.13333334,\n",
              "       0.07407407, 0.24089758, 0.03333334, 0.13333334, 0.32955346,\n",
              "       0.1       , 0.11111111, 0.19307889, 0.46407142, 0.        ,\n",
              "       0.15593928, 0.33474764, 0.        , 0.06666667, 0.29677227,\n",
              "       0.35607985, 0.        , 0.54397428, 0.15151516, 0.06666667,\n",
              "       0.        , 0.23481071, 0.        , 0.13333334, 0.72546297,\n",
              "       0.28323513, 0.42999083, 0.13333334, 0.27619246, 0.34606311,\n",
              "       0.35027498, 0.6118232 , 0.        , 0.21534488, 0.1       ,\n",
              "       0.06666667, 0.93399435, 0.1       , 0.37129858, 0.6233272 ,\n",
              "       0.19740476, 0.11586908, 0.41618383, 0.15554   , 0.25502828,\n",
              "       0.        , 0.1       , 0.41773459, 0.03030303, 0.1       ])"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_test_top_100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9uqHZ5fa7uq"
      },
      "source": [
        "# **Rank Net Ranking Loss, with PyTorch**\n",
        "\n",
        "Given a set of training examples $ \\{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\\} $, we consider all possible **ordered pairs** $ (i, j) $ where $ i \\neq j $, and we compute the **pairwise loss**:\n",
        "\n",
        "$$\n",
        "L = \\sum_{i < j} - y_{ij} \\log P(i > j) - (1 - y_{ij}) \\log (1 - P(i > j))\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "- $ y_{ij} = 1 $ if $ x_i $ should be ranked higher than $ x_j $, otherwise $ y_{ij} = 0 $.\n",
        "- **In here since all true labels has prob 1, we only consider the loss between true and false ones**.\n",
        "- $ P(i > j) = \\frac{1}{1 + e^{-(s(x_i) - s(x_j))}} $ is the probability that $ x_i $ is ranked higher than $ x_j $. **s is the rankning score assigned to i, higher s higher ranking.**\n",
        "- The sum runs over all possible pairs $ (i, j) $ with $ i < j $.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hvAaRoabCSd",
        "outputId": "02494a2f-11d8-42a4-8593-727d0c6a5c1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 50, Avg Loss: 0.3647\n",
            "Epoch 100, Avg Loss: 0.2995\n",
            "Epoch 150, Avg Loss: 0.2679\n",
            "Epoch 200, Avg Loss: 0.2537\n",
            "Epoch 250, Avg Loss: 0.2490\n",
            "Epoch 300, Avg Loss: 0.2415\n",
            "Epoch 350, Avg Loss: 0.2376\n",
            "Epoch 400, Avg Loss: 0.2458\n",
            "Epoch 450, Avg Loss: 0.2220\n",
            "Epoch 500, Avg Loss: 0.2551\n",
            "    Sample  True Class  Predicted Score\n",
            "0      596    0.833914        13.416726\n",
            "1      997    0.352183        10.396317\n",
            "2      197    0.000000        10.390798\n",
            "3      624    0.372656        10.267813\n",
            "4      141    0.608551         9.348565\n",
            "..     ...         ...              ...\n",
            "95     650    0.887296         5.348153\n",
            "96     662    0.433943         5.329562\n",
            "97     855    0.233781         5.326482\n",
            "98     929    0.292134         5.321490\n",
            "99     106    0.167950         5.302687\n",
            "\n",
            "[100 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "from itertools import combinations\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# Create DataLoader for batching\n",
        "batch_size = 256\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Define a simple neural network for ranking (1 hidden layer, 128 units)\n",
        "class RankNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=128):\n",
        "        super(RankNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, 1)  # Output raw ranking scores\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x  # Output raw scores (no Sigmoid, because we use ranking loss)\n",
        "\n",
        "# Use PyTorch's built-in margin ranking loss\n",
        "def ranknet_loss(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    Efficient pairwise RankNet loss using margin ranking loss.\n",
        "    - y_pred: Predicted scores\n",
        "    - y_true: Ground truth binary labels\n",
        "    \"\"\"\n",
        "    indices = torch.arange(len(y_true))  # Index all elements in the batch\n",
        "    sampled_pairs = list(combinations(indices.tolist(), 2))  # Sample pairs\n",
        "\n",
        "    if not sampled_pairs:\n",
        "        return torch.tensor(0.0, requires_grad=True)\n",
        "\n",
        "    i, j = zip(*sampled_pairs)  # Unpack sampled pairs\n",
        "    i, j = torch.tensor(i), torch.tensor(j)\n",
        "\n",
        "    y_i, y_j = y_true[i], y_true[j]\n",
        "    pred_i, pred_j = y_pred[i], y_pred[j]\n",
        "\n",
        "    # valid_pairs = (y_i != y_j)  # Only keep pairs with different labels\n",
        "    # if valid_pairs.sum() == 0:\n",
        "    #     return torch.tensor(0.0, requires_grad=True)\n",
        "\n",
        "    y_target = torch.where(y_i > y_j, torch.ones_like(y_i), -torch.ones_like(y_i))  # Set target for ranking loss\n",
        "\n",
        "    return nn.MarginRankingLoss(margin=1.0)(pred_i, pred_j, y_target)  # Optimized ranking loss\n",
        "\n",
        "# Initialize model, optimizer\n",
        "model = RankNet(input_dim=40, hidden_dim=128)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "# Training loop\n",
        "epochs = 500\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        y_pred = model(X_batch).squeeze()  # Get predicted scores\n",
        "        loss = ranknet_loss(y_pred, y_batch)  # Compute ranking loss\n",
        "\n",
        "        if loss.requires_grad:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    if (epoch + 1) % 50 == 0:\n",
        "        print(f\"Epoch {epoch+1}, Avg Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "# Evaluate model and rank samples\n",
        "model.eval()\n",
        "y_pred_scores = model(X_test_tensor).detach().numpy().flatten()  # Get predicted scores\n",
        "ranking = np.argsort(-y_pred_scores)  # Rank in descending order\n",
        "\n",
        "# Create ranked DataFrame\n",
        "ranked_df = pd.DataFrame({'Sample': np.arange(len(y_test)), 'True Class': y_test, 'Predicted Score': y_pred_scores})\n",
        "ranked_df = ranked_df.iloc[ranking].reset_index(drop=True)\n",
        "print(ranked_df.head(100))  # Show top-ranked samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yn3viSXiBvR_",
        "outputId": "d5517967-9275-4c53-ad8e-958a7aac8d56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 50, Avg Loss: 0.1724\n",
            "    Sample  True Class  Predicted Score\n",
            "0      596    0.833914         5.659933\n",
            "1      370    1.000000         4.134814\n",
            "2      151    0.226369         4.130959\n",
            "3      110    0.740053         4.049060\n",
            "4       15    1.000000         3.839596\n",
            "..     ...         ...              ...\n",
            "95      21    0.100000         1.661579\n",
            "96     699    0.216813         1.652558\n",
            "97     865    0.553375         1.636178\n",
            "98     210    0.369869         1.634777\n",
            "99     176    0.696001         1.625689\n",
            "\n",
            "[100 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# Create DataLoader for batching\n",
        "batch_size = 128\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Define a simple neural network for ranking (1 hidden layer, 128 units)\n",
        "class RankNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=128):\n",
        "        super(RankNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, 1)  # Output raw ranking scores\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x  # Output raw scores (no Sigmoid, because we use ranking loss)\n",
        "\n",
        "# Use PyTorch's built-in margin ranking loss\n",
        "def ranknet_loss(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    Efficient pairwise RankNet loss using margin ranking loss.\n",
        "    - y_pred: Predicted scores\n",
        "    - y_true: Ground truth binary labels\n",
        "    \"\"\"\n",
        "    n = len(y_true)\n",
        "    indices = torch.arange(n)  # Index all elements in the batch\n",
        "\n",
        "    # Generate valid pairs (y_i != y_j)\n",
        "    sampled_pairs = [(i, j) for i, j in combinations(indices.tolist(), 2) if y_true[i] != y_true[j]]\n",
        "\n",
        "    if len(sampled_pairs) == 0:\n",
        "        return torch.tensor(0.0, requires_grad=True)\n",
        "\n",
        "    i, j = zip(*sampled_pairs)  # Unpack sampled pairs\n",
        "    i, j = torch.tensor(i), torch.tensor(j)\n",
        "\n",
        "    y_i, y_j = y_true[i], y_true[j]\n",
        "    pred_i, pred_j = y_pred[i], y_pred[j]\n",
        "\n",
        "    # Target labels for ranking loss: +1 if y_i > y_j, else -1\n",
        "    y_target = torch.where(y_i > y_j, torch.ones_like(y_i), -torch.ones_like(y_i))\n",
        "\n",
        "    # Use MarginRankingLoss\n",
        "    return nn.MarginRankingLoss(margin=1.0)(pred_i, pred_j, y_target)  # Optimized ranking loss\n",
        "\n",
        "# Initialize model, optimizer\n",
        "model = RankNet(input_dim=40, hidden_dim=128)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "# Training loop\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        y_pred = model(X_batch).squeeze()  # Get predicted scores\n",
        "        loss = ranknet_loss(y_pred, y_batch)  # Compute ranking loss\n",
        "\n",
        "        if loss.requires_grad:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    if (epoch + 1) % 50 == 0:\n",
        "        print(f\"Epoch {epoch+1}, Avg Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "# Evaluate model and rank samples\n",
        "model.eval()\n",
        "y_pred_scores = model(X_test_tensor).detach().numpy().flatten()  # Get predicted scores\n",
        "ranking = np.argsort(-y_pred_scores)  # Rank in descending order\n",
        "\n",
        "# Create ranked DataFrame\n",
        "ranked_df = pd.DataFrame({'Sample': np.arange(len(y_test)), 'True Class': y_test, 'Predicted Score': y_pred_scores})\n",
        "ranked_df = ranked_df.iloc[ranking].reset_index(drop=True)\n",
        "print(ranked_df.head(100))  # Show top-ranked samples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0nJSvBCle-1",
        "outputId": "74f1cc82-f9ba-45a0-dff7-dc2581329d55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "37\n",
            "150.91162309609354\n"
          ]
        }
      ],
      "source": [
        "print((ranked_df.head(100)[\"True Class\"] >= 0.5).sum())\n",
        "print(y_test.sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "UBBGKJusODej",
        "outputId": "55da8185-084d-4f8c-f123-be4692530b1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split 1/10\n",
            "Split 2/10\n",
            "Split 3/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-ca84f3c95b52>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Get predicted scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mranknet_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Compute ranking loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-ca84f3c95b52>\u001b[0m in \u001b[0;36mranknet_loss\u001b[0;34m(y_pred, y_true)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# Generate valid pairs where y_true[i] != y_true[j]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0msampled_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_pairs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-ca84f3c95b52>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# Generate valid pairs where y_true[i] != y_true[j]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0msampled_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_pairs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from itertools import combinations\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define simple neural network for ranking (RankNet)\n",
        "class RankNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=128):\n",
        "        super(RankNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, 1)  # Output raw ranking scores\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x  # Output raw scores (no Sigmoid, because we use ranking loss)\n",
        "\n",
        "# RankNet loss\n",
        "# def ranknet_loss(y_pred, y_true):\n",
        "#     n = len(y_true)\n",
        "#     indices = torch.arange(n)  # Index all elements in the batch\n",
        "\n",
        "#     # Generate valid pairs (y_i != y_j)\n",
        "#     sampled_pairs = [(i, j) for i, j in combinations(indices.tolist(), 2)]\n",
        "\n",
        "#     if len(sampled_pairs) == 0:\n",
        "#         return torch.tensor(0.0, requires_grad=True)\n",
        "\n",
        "#     i, j = zip(*sampled_pairs)  # Unpack sampled pairs\n",
        "#     i, j = torch.tensor(i), torch.tensor(j)\n",
        "\n",
        "#     y_i, y_j = y_true[i], y_true[j]\n",
        "#     pred_i, pred_j = y_pred[i], y_pred[j]\n",
        "\n",
        "#     # Target labels for ranking loss: +1 if y_i > y_j, else -1\n",
        "#     y_target = torch.where(y_i > y_j, torch.ones_like(y_i), -torch.ones_like(y_i))\n",
        "\n",
        "#     # Use MarginRankingLoss\n",
        "#     return nn.MarginRankingLoss(margin=1.0)(pred_i, pred_j, y_target)  # Optimized ranking loss\n",
        "\n",
        "def ranknet_loss(y_pred, y_true):\n",
        "    n = len(y_true)\n",
        "    indices = torch.arange(n)\n",
        "\n",
        "    # Generate valid pairs where y_true[i] != y_true[j]\n",
        "    sampled_pairs = [(i, j) for i, j in combinations(indices.tolist(), 2) if y_true[i] != y_true[j]]\n",
        "\n",
        "    if len(sampled_pairs) == 0:\n",
        "        return torch.tensor(0.0, requires_grad=True)\n",
        "\n",
        "    # Unpack the sampled pairs\n",
        "    i, j = zip(*sampled_pairs)\n",
        "    i = torch.tensor(i)\n",
        "    j = torch.tensor(j)\n",
        "\n",
        "    # Get predicted scores and true labels for the pairs\n",
        "    s_i, s_j = y_pred[i], y_pred[j]\n",
        "    y_i, y_j = y_true[i], y_true[j]\n",
        "\n",
        "    # Binary target: 1 if y_i > y_j, else 0\n",
        "    target = (y_i > y_j).float()\n",
        "\n",
        "    # Score difference\n",
        "    diff = s_i - s_j\n",
        "\n",
        "    # Compute RankNet loss using binary cross-entropy with logits\n",
        "    loss = F.binary_cross_entropy_with_logits(diff, target)\n",
        "\n",
        "    return loss\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "random_seed = 42\n",
        "\n",
        "# Number of splits (10)\n",
        "num_splits = 10\n",
        "mlp_correct_predictions = []\n",
        "ranknet_correct_predictions = []\n",
        "\n",
        "for split in range(num_splits):\n",
        "    print(f\"Split {split+1}/{num_splits}\")\n",
        "\n",
        "    # Split the data with different random states\n",
        "    random_state = random_seed + split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state)\n",
        "\n",
        "    # Train and evaluate MLP model\n",
        "    mlp = MLPClassifier(hidden_layer_sizes=(128,), activation='relu', solver='adam', max_iter=800, random_state=random_state)\n",
        "    y_train_class = y_train > label_bin_cutoff\n",
        "    mlp.fit(X_train, y_train_class)\n",
        "\n",
        "    # Predict probabilities for the test set\n",
        "    y_pred_probs = mlp.predict_proba(X_test)[:, 1]  # Probabilities for class 1\n",
        "\n",
        "    # Get the top 100 samples with the highest probabilities\n",
        "    top_100_indices = np.argsort(-y_pred_probs)[:100]  # Sort in descending order\n",
        "    top_100_labels = y_test[top_100_indices]  # Get actual class labels of top 100\n",
        "\n",
        "    # Count how many of the top 100 are actually class 1\n",
        "    num_class_1_mlp = np.sum(top_100_labels > 0.5)\n",
        "    mlp_correct_predictions.append(num_class_1_mlp)\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "    # Create DataLoader for batching\n",
        "    batch_size = 128\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Initialize RankNet model\n",
        "    model = RankNet(input_dim=X_train.shape[1], hidden_dim=128)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "    # Training loop\n",
        "    epochs = 80\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            y_pred = model(X_batch).squeeze()  # Get predicted scores\n",
        "            loss = ranknet_loss(y_pred, y_batch)  # Compute ranking loss\n",
        "\n",
        "            if loss.requires_grad:\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "\n",
        "    # Evaluate RankNet model and rank samples\n",
        "    model.eval()\n",
        "    y_pred_scores = model(X_test_tensor).detach().numpy().flatten()  # Get predicted scores\n",
        "    ranking = np.argsort(-y_pred_scores)  # Rank in descending order\n",
        "\n",
        "    # Get the top 100 ranked samples\n",
        "    ranked_top_100_labels = y_test[ranking[:100]]  # Get actual class labels of top 100\n",
        "    num_class_1_ranknet = np.sum(ranked_top_100_labels > 0.5)\n",
        "    ranknet_correct_predictions.append(num_class_1_ranknet)\n",
        "\n",
        "# Report results\n",
        "print(\"\\nPerformance across 10 splits:\")\n",
        "\n",
        "# MLP Model: number of class 1 predictions in top 100 for each split\n",
        "print(f\"MLP model - Correct predictions in top 100:\")\n",
        "for i, correct in enumerate(mlp_correct_predictions, 1):\n",
        "    print(f\"Split {i}: {correct} correct\")\n",
        "\n",
        "# RankNet Model: number of class 1 predictions in top 100 for each split\n",
        "print(f\"\\nRankNet model - Correct predictions in top 100:\")\n",
        "for i, correct in enumerate(ranknet_correct_predictions, 1):\n",
        "    print(f\"Split {i}: {correct} correct\")\n",
        "\n",
        "# Average performance across all splits\n",
        "print(f\"\\nMLP Average Correct Predictions in Top 100: {np.mean(mlp_correct_predictions)}\")\n",
        "print(f\"RankNet Average Correct Predictions in Top 100: {np.mean(ranknet_correct_predictions)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from itertools import combinations\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define RankNet model and its loss function\n",
        "class RankNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=128):\n",
        "        super(RankNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, 1)  # Output raw ranking scores\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x  # Raw scores\n",
        "\n",
        "def ranknet_loss(y_pred, y_true):\n",
        "    n = len(y_true)\n",
        "    indices = torch.arange(n)\n",
        "    # Generate valid pairs where labels differ\n",
        "    sampled_pairs = [(i, j) for i, j in combinations(indices.tolist(), 2) if y_true[i] != y_true[j]]\n",
        "    if len(sampled_pairs) == 0:\n",
        "        return torch.tensor(0.0, requires_grad=True)\n",
        "    i, j = zip(*sampled_pairs)\n",
        "    i = torch.tensor(i)\n",
        "    j = torch.tensor(j)\n",
        "    s_i, s_j = y_pred[i], y_pred[j]\n",
        "    y_i, y_j = y_true[i], y_true[j]\n",
        "    target = (y_i > y_j).float()  # Binary target: 1 if y_i > y_j else 0\n",
        "    diff = s_i - s_j\n",
        "    loss = F.binary_cross_entropy_with_logits(diff, target)\n",
        "    return loss\n",
        "\n",
        "# Define a simple MLP for focal loss training\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=64):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, 1)  # Output a single logit\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x  # Raw logits\n",
        "\n",
        "# Implement Focal Loss for binary classification\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.25, gamma=2.0):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        targets = targets.view(-1)  # Ensure shape (batch_size,)\n",
        "        probs = torch.sigmoid(logits)\n",
        "        bce_loss = self.bce(logits, targets)\n",
        "        pt = probs * targets + (1 - probs) * (1 - targets)  # p_t\n",
        "        focal_weight = (1 - pt) ** self.gamma\n",
        "        alpha_factor = self.alpha * targets + (1 - self.alpha) * (1 - targets)\n",
        "        focal_loss = alpha_factor * focal_weight * bce_loss\n",
        "        return focal_loss.mean()\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "random_seed = 42\n",
        "\n",
        "# Number of splits (10)\n",
        "num_splits = 10\n",
        "mlp_correct_predictions = []\n",
        "ranknet_correct_predictions = []\n",
        "focal_loss_correct_predictions = []\n",
        "\n",
        "for split in range(num_splits):\n",
        "    print(f\"\\n===== Split {split+1}/{num_splits} =====\")\n",
        "\n",
        "    # Split the data with different random states\n",
        "    random_state = random_seed + split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state)\n",
        "\n",
        "    # Create binary labels for classification based on your cutoff\n",
        "    y_train_class = (y_train > label_bin_cutoff).astype(np.float32)\n",
        "    y_test_class = (y_test > label_bin_cutoff).astype(np.float32)\n",
        "\n",
        "    # ----- Sklearn MLP Model -----\n",
        "    mlp = MLPClassifier(hidden_layer_sizes=(128,), activation='relu', solver='adam', max_iter=800, random_state=random_state)\n",
        "    mlp.fit(X_train, y_train_class)\n",
        "    y_pred_probs = mlp.predict_proba(X_test)[:, 1]  # Probabilities for class 1\n",
        "    top_100_indices = np.argsort(-y_pred_probs)[:100]\n",
        "    top_100_labels = y_test[top_100_indices]\n",
        "    num_class_1_mlp = np.sum(top_100_labels > 0.5)\n",
        "    mlp_correct_predictions.append(num_class_1_mlp)\n",
        "\n",
        "    # ----- RankNet Model -----\n",
        "    # Convert training and test data to torch tensors for RankNet\n",
        "    X_train_tensor_rank = torch.tensor(X_train, dtype=torch.float32)\n",
        "    y_train_tensor_rank = torch.tensor(y_train, dtype=torch.float32)\n",
        "    X_test_tensor_rank = torch.tensor(X_test, dtype=torch.float32)\n",
        "    y_test_tensor_rank = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "    batch_size = 128\n",
        "    train_dataset_rank = TensorDataset(X_train_tensor_rank, y_train_tensor_rank)\n",
        "    train_loader_rank = DataLoader(train_dataset_rank, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    model_rank = RankNet(input_dim=X_train.shape[1], hidden_dim=128)\n",
        "    optimizer_rank = optim.Adam(model_rank.parameters(), lr=0.005)\n",
        "    epochs_rank = 100\n",
        "    for epoch in range(epochs_rank):\n",
        "        model_rank.train()\n",
        "        total_loss_rank = 0\n",
        "        for X_batch, y_batch in train_loader_rank:\n",
        "            optimizer_rank.zero_grad()\n",
        "            y_pred = model_rank(X_batch).squeeze()\n",
        "            loss = ranknet_loss(y_pred, y_batch)\n",
        "            if loss.requires_grad:\n",
        "                loss.backward()\n",
        "                optimizer_rank.step()\n",
        "                total_loss_rank += loss.item()\n",
        "    model_rank.eval()\n",
        "    y_pred_scores = model_rank(X_test_tensor_rank).detach().numpy().flatten()\n",
        "    ranking = np.argsort(-y_pred_scores)\n",
        "    ranked_top_100_labels = y_test[ranking[:100]]\n",
        "    num_class_1_ranknet = np.sum(ranked_top_100_labels > 0.5)\n",
        "    ranknet_correct_predictions.append(num_class_1_ranknet)\n",
        "\n",
        "    # ----- Focal Loss Model -----\n",
        "    # Convert training and test data to torch tensors for focal loss model\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "    y_train_tensor_class = torch.tensor(y_train_class, dtype=torch.float32)\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "    y_test_tensor_class = torch.tensor(y_test_class, dtype=torch.float32)\n",
        "\n",
        "    model_focal = MLP(input_dim=X_train.shape[1], hidden_dim=64)\n",
        "    optimizer_focal = optim.Adam(model_focal.parameters(), lr=0.001)\n",
        "    criterion_focal = FocalLoss(alpha=0.25, gamma=2.0)\n",
        "\n",
        "    epochs_focal = 100\n",
        "    focal_batch_size = 128\n",
        "    train_dataset_focal = TensorDataset(X_train_tensor, y_train_tensor_class)\n",
        "    train_loader_focal = DataLoader(train_dataset_focal, batch_size=focal_batch_size, shuffle=True)\n",
        "\n",
        "    for epoch in range(epochs_focal):\n",
        "        model_focal.train()\n",
        "        total_loss_focal = 0\n",
        "        for X_batch, y_batch in train_loader_focal:\n",
        "            optimizer_focal.zero_grad()\n",
        "            logits = model_focal(X_batch).squeeze()\n",
        "            loss = criterion_focal(logits, y_batch.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer_focal.step()\n",
        "            total_loss_focal += loss.item()\n",
        "        if (epoch + 1) % 50 == 0:\n",
        "            avg_loss = total_loss_focal / len(train_loader_focal)\n",
        "            print(f\"Focal Loss Model - Epoch {epoch+1}, Avg Train Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    model_focal.eval()\n",
        "    with torch.no_grad():\n",
        "        logits_test = model_focal(X_test_tensor).squeeze()\n",
        "        probs_test = torch.sigmoid(logits_test).numpy()\n",
        "\n",
        "    top_100_indices_focal = np.argsort(-probs_test)[:100]\n",
        "    top_100_labels_focal = y_test_class[top_100_indices_focal]\n",
        "    num_class_1_focal = np.sum(top_100_labels_focal > 0.5)\n",
        "    focal_loss_correct_predictions.append(num_class_1_focal)\n",
        "\n",
        "# ----- Reporting Results -----\n",
        "print(\"\\nPerformance across 10 splits:\")\n",
        "\n",
        "print(\"\\nSklearn MLP Model - Correct predictions in top 100:\")\n",
        "for i, correct in enumerate(mlp_correct_predictions, 1):\n",
        "    print(f\"Split {i}: {correct} correct\")\n",
        "\n",
        "print(\"\\nRankNet Model - Correct predictions in top 100:\")\n",
        "for i, correct in enumerate(ranknet_correct_predictions, 1):\n",
        "    print(f\"Split {i}: {correct} correct\")\n",
        "\n",
        "print(\"\\nFocal Loss Model - Correct predictions in top 100:\")\n",
        "for i, correct in enumerate(focal_loss_correct_predictions, 1):\n",
        "    print(f\"Split {i}: {correct} correct\")\n",
        "\n",
        "print(f\"\\nMLP Average Correct Predictions in Top 100: {np.mean(mlp_correct_predictions)}\")\n",
        "print(f\"RankNet Average Correct Predictions in Top 100: {np.mean(ranknet_correct_predictions)}\")\n",
        "print(f\"Focal Loss Average Correct Predictions in Top 100: {np.mean(focal_loss_correct_predictions)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KlBDDz1gasO",
        "outputId": "a912674d-b3c7-4c2d-85a4-66dc3c602559"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Split 1/10 =====\n",
            "Focal Loss Model - Epoch 50, Avg Train Loss: 0.0116\n",
            "Focal Loss Model - Epoch 100, Avg Train Loss: 0.0071\n",
            "\n",
            "===== Split 2/10 =====\n",
            "Focal Loss Model - Epoch 50, Avg Train Loss: 0.0108\n",
            "Focal Loss Model - Epoch 100, Avg Train Loss: 0.0064\n",
            "\n",
            "===== Split 3/10 =====\n",
            "Focal Loss Model - Epoch 50, Avg Train Loss: 0.0118\n",
            "Focal Loss Model - Epoch 100, Avg Train Loss: 0.0077\n",
            "\n",
            "===== Split 4/10 =====\n",
            "Focal Loss Model - Epoch 50, Avg Train Loss: 0.0126\n",
            "Focal Loss Model - Epoch 100, Avg Train Loss: 0.0074\n",
            "\n",
            "===== Split 5/10 =====\n",
            "Focal Loss Model - Epoch 50, Avg Train Loss: 0.0100\n",
            "Focal Loss Model - Epoch 100, Avg Train Loss: 0.0053\n",
            "\n",
            "===== Split 6/10 =====\n",
            "Focal Loss Model - Epoch 50, Avg Train Loss: 0.0116\n",
            "Focal Loss Model - Epoch 100, Avg Train Loss: 0.0070\n",
            "\n",
            "===== Split 7/10 =====\n",
            "Focal Loss Model - Epoch 50, Avg Train Loss: 0.0102\n",
            "Focal Loss Model - Epoch 100, Avg Train Loss: 0.0057\n",
            "\n",
            "===== Split 8/10 =====\n",
            "Focal Loss Model - Epoch 50, Avg Train Loss: 0.0115\n",
            "Focal Loss Model - Epoch 100, Avg Train Loss: 0.0063\n",
            "\n",
            "===== Split 9/10 =====\n",
            "Focal Loss Model - Epoch 50, Avg Train Loss: 0.0110\n",
            "Focal Loss Model - Epoch 100, Avg Train Loss: 0.0062\n",
            "\n",
            "===== Split 10/10 =====\n",
            "Focal Loss Model - Epoch 50, Avg Train Loss: 0.0118\n",
            "Focal Loss Model - Epoch 100, Avg Train Loss: 0.0076\n",
            "\n",
            "Performance across 10 splits:\n",
            "\n",
            "Sklearn MLP Model - Correct predictions in top 100:\n",
            "Split 1: 36 correct\n",
            "Split 2: 34 correct\n",
            "Split 3: 38 correct\n",
            "Split 4: 40 correct\n",
            "Split 5: 44 correct\n",
            "Split 6: 39 correct\n",
            "Split 7: 42 correct\n",
            "Split 8: 37 correct\n",
            "Split 9: 40 correct\n",
            "Split 10: 42 correct\n",
            "\n",
            "RankNet Model - Correct predictions in top 100:\n",
            "Split 1: 34 correct\n",
            "Split 2: 35 correct\n",
            "Split 3: 33 correct\n",
            "Split 4: 40 correct\n",
            "Split 5: 44 correct\n",
            "Split 6: 40 correct\n",
            "Split 7: 44 correct\n",
            "Split 8: 39 correct\n",
            "Split 9: 42 correct\n",
            "Split 10: 37 correct\n",
            "\n",
            "Focal Loss Model - Correct predictions in top 100:\n",
            "Split 1: 35 correct\n",
            "Split 2: 38 correct\n",
            "Split 3: 39 correct\n",
            "Split 4: 41 correct\n",
            "Split 5: 50 correct\n",
            "Split 6: 40 correct\n",
            "Split 7: 43 correct\n",
            "Split 8: 40 correct\n",
            "Split 9: 39 correct\n",
            "Split 10: 41 correct\n",
            "\n",
            "MLP Average Correct Predictions in Top 100: 39.2\n",
            "RankNet Average Correct Predictions in Top 100: 38.8\n",
            "Focal Loss Average Correct Predictions in Top 100: 40.6\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}